{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI4022 Homework 2; Review\n",
    "\n",
    "## Due Monday, February 8 at 11:59 pm to Canvas\n",
    "\n",
    "#### Submit this file as a .ipynb with *all cells compiled and run* to the associated dropbox.\n",
    "\n",
    "***\n",
    "\n",
    "Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Here is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.  I also recommend the [wikibook](https://en.wikibooks.org/wiki/LaTeX) for LaTex.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do **Kernel $\\rightarrow$ Restart & Run All** as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 45 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "- There is *not a prescribed API* for these problems.  You may answer coding questions with whatever syntax or object typing you deem fit.  Your evaluation will primarily live in the clarity of how well you present your final results, so don't skip over any interpretations!  Your code should still be commented and readable to ensure you followed the given course algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p1'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 1 (Theory: minhashing; 10 pts)\n",
    "\n",
    "Consider minhash values for a single column vector that contains 10 components/rows. Seven of rows hold 0 and three hold 1. Consider taking all 10! = 3,628,800 possible distinct permutations of ten rows. When we choose a permutation of the rows and produce a minhash value for the column, we will use the number of the row, in the permuted order, that is the first with a 1.  Use Markdown cells to demonstrate answers to the following.\n",
    "\n",
    "#### a) For exactly how many of the 3,628,800 permutations is the minhash value for the column a 9?  What proportion is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets process through this if each permutation shuffles the order, any which order still has to contain 3 1's. Let's attempt to \"pick\"  a permutaion that creates a minhash value for column 9. It is hard because the way minhash works is it will take row with the first 1. Hence a permutation that minhashes to a 9 is impossible because we will need one less row that holds a 1 and we cant get rid of a 1.  \n",
    "- 000 000 0011\n",
    "\n",
    "Hence the proportion is 0/3628800 permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) For exactly how many of the 3,628,800 permutations is the minhash value for the column a 8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is exactly 6 permutation that minhash to value 8. That permutaion is [ 0000000 111 ] as the first 1 would be the 8th index hence the value. However, by definition of permutation order matters so we have to acount all the different ways we can place the last 3 ones in the last 3 spots which is 6 ways and since the rest of the spots are 0's we need to multiply by $2^7$ to fill the open spots.\n",
    "- $6 \\times 2^7 = 768 $ times\n",
    "\n",
    "Hence the proprtion is $6*2^7/362880 = 768/362880 \\approx 2.12 \\%$ of permutations map out to 8. (its 6*2^7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) For exactly how many of the 3,628,800 permutations is the minhash value for the column a 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this like part b. The only value in any given permutation will need to have the first 1 be in accordance to the minhash value. So the permutation must start with 001... So it will be the same as b however we need to account selecting 1 one and two 0's to put in the correct order. Hence the number of permutations that map out to 3 is given below:\n",
    "- $6 \\times 2^7 \\times P(7,2) \\times P(3,1) = 6 \\times 2^7 \\times 42 \\times 3 = 96768 $ times\n",
    "\n",
    "Hence the porportion is $96768/362880 \\approx 26.67\\% $ of permutations map out to 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p3'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 2 (Applied Minhashing; 35 pts)\n",
    "\n",
    "In this problem we compare similarities of 5 documents available on http://www.gutenberg.org\n",
    "\n",
    " 1) The first approximately 10000 characters of Miguel de Unamuno's *Niebla*, written in Spanish, in the file `niebla.txt`\n",
    " \n",
    " 2) The first approximately 10000 characters of Miguel de Cervantes *The Ingenious Gentleman Don Quixote of La Mancha*, written in Spanish, in the file `DQ.txt`\n",
    " \n",
    " 3) The first approximately 10000 characters of Homer's *The Odyssey*, translated into English by Samuel Butler, in the file `odyssey.txt`\n",
    " \n",
    " 4) The first approximately 10000 characters of Kate Chopin's *The Awakening* in the file `awaken.txt`\n",
    " \n",
    " 5) The entirety of around 12000 characters of Kate Chopin's *Beyond the Bayou* in the file `BB.txt`\n",
    " \n",
    "### a) Clean the 4 documents, scrubbing all punctuation, changes cases to lower case, and removing accent marks as appropriate.  \n",
    "\n",
    "You should have only 27 unique characters in each book/section after cleaning, corresponding to white spaces and the 26 letters.  \n",
    "\n",
    "\n",
    "**For this problem, you may import any text-based packages you desire to help wrangle the data.**  I recommend looking at some functions within `string` or the RegEx `re` packages.\n",
    "\n",
    "You can and probably should use functions in the string package such as `string.lower`, `string.replace`, etc.\n",
    "\n",
    "All 5 documents have been saved in UTF-8 encoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "def cleanDoc(fileName):\n",
    "    clean_Doc =\"\"\n",
    "    regEx_p= ['[a-z]+']\n",
    "    delimeter = ' '\n",
    "    with open(fileName, 'r',encoding='utf-8') as reader:\n",
    "        # Read and print the entire file line by line\n",
    "        #reader =reader.lower()\n",
    "        for line in reader:\n",
    "            #if(line !=\"\"):\n",
    "            line = line.lower()\n",
    "            line = line.replace('\\'','')\n",
    "            line = line.replace(',','')\n",
    "            line = line.replace('(','')\n",
    "            line = line.replace(')','')\n",
    "            line = line.replace('.','')\n",
    "            line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore').decode(\"utf-8\") # will get rid of all accent marks\n",
    "            #for just getting a-z\n",
    "            for p in regEx_p:\n",
    "                line_arr= re.findall(p, line)\n",
    "                if(len(line_arr)!=0):\n",
    "                    line = delimeter.join(line_arr)\n",
    "                    #print(line, end='')\n",
    "                    #print(\"\")\n",
    "                    clean_Doc = clean_Doc + \" \"+ line\n",
    "            #print(line, end='')\n",
    "\n",
    "    return clean_Doc\n",
    "niebla = cleanDoc(\"niebla.txt\")\n",
    "dq = cleanDoc(\"DQ.txt\")\n",
    "odyssey =cleanDoc(\"odyssey.txt\")\n",
    "awaken =cleanDoc(\"awaken.txt\")\n",
    "bb = cleanDoc(\"BB.txt\")\n",
    "#print(len(cleanDoc(\"niebla.txt\")))\n",
    "#print(len(cleanDoc(\"DQ.txt\")))\n",
    "#print(cleanDoc(\"DQ.txt\"))\n",
    "#print(len(cleanDoc(\"BB.txt\")))\n",
    "#print(cleanDoc(\"BB.txt\"))\n",
    "#print(len(cleanDoc(\"awaken.txt\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b) Compute exact similarity scores between the documents.  Are these the expected results?\n",
    "\n",
    "Notes:\n",
    "- You may choose or explore different values of $k$ for your shingles.\n",
    "- You may choose to shingle on words and create an n-gram model, but it is recommended you shingle on letters as described in class\n",
    "- You may construct your characteristic matrix or characteristic sets with or without hash functions (e.g. by using `set()`).  Note that choice of hash function should change heavily with $k$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4  #5 or 9\n",
    "def getDoc(doc,k):\n",
    "    shingleList =[]\n",
    "    for i in range(1,len(doc)):\n",
    "        shingleList.append(doc[i-1:i+k-1])\n",
    "        #print(doc[i-1:i+k-1])\n",
    "    return set(shingleList)\n",
    "def getCharacteristic_Col(charMatrix,shingleSet):\n",
    "    simList = []\n",
    "    bigList =charMatrix.iloc[:, 0]\n",
    "    for i in range(len(bigList)):\n",
    "        #if(i<4):\n",
    "        word =bigList[i]\n",
    "        tempSet = {word}\n",
    "        #print(tempSet)\n",
    "        boolAns = tempSet.issubset(shingleSet)\n",
    "        intAns = int(boolAns)\n",
    "        simList.append(intAns)\n",
    "    \n",
    "    #print(simList)\n",
    "    return simList\n",
    "nieble_shingles =getDoc(niebla,k)\n",
    "dq_shingles =getDoc(dq,k)\n",
    "odyssey_shingles =getDoc(odyssey,k)\n",
    "awaken_shingles =getDoc(awaken,k)\n",
    "bb_shingles =getDoc(bb,k)\n",
    "\n",
    "bigList = nieble_shingles | dq_shingles | odyssey_shingles | awaken_shingles|bb_shingles\n",
    "\n",
    "\n",
    "#create characteristic matrix\n",
    "charMatrix = pd.DataFrame(list(bigList),columns=['shingle'])\n",
    "#create function to go through shingle list and check with each one in df\n",
    "nieble_simList = getCharacteristic_Col(charMatrix,nieble_shingles)\n",
    "dq_simList = getCharacteristic_Col(charMatrix,dq_shingles)\n",
    "odyssey_simList = getCharacteristic_Col(charMatrix,odyssey_shingles)\n",
    "awaken_simList = getCharacteristic_Col(charMatrix,awaken_shingles)\n",
    "bb_simList = getCharacteristic_Col(charMatrix,bb_shingles)\n",
    "#add each to characteristic matrix\n",
    "charMatrix['Nieble']= nieble_simList\n",
    "charMatrix['DQ']= dq_simList\n",
    "charMatrix['Odyssey']= odyssey_simList\n",
    "charMatrix['Awaken']= awaken_simList\n",
    "charMatrix['BB']= bb_simList\n",
    "\n",
    "#charMatrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_charMatrix = charMatrix.iloc[:, 1:].copy()\n",
    "#df_charMatrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nieble': 1.0, 'DQ': 0.2853838065194532, 'Odyssey': 0.09849624060150376, 'Awaken': 0.10057769219374907, 'BB': 0.10232886379675371}\n"
     ]
    }
   ],
   "source": [
    "#single case for practice\n",
    "sims = {}\n",
    "docName = \"Nieble\"\n",
    "for user2 in df_charMatrix.columns:\n",
    "    #sims = {}\n",
    "    #for user2 in df_charMatrix.columns:\n",
    "    col1 = docName\n",
    "    col2 = user2\n",
    "    numer = np.sum((df_charMatrix[col1]==1) & (df_charMatrix[col2]==1))\n",
    "    denom = np.sum((df_charMatrix[col1]==1) | (df_charMatrix[col2]==1))\n",
    "    sim = numer/denom\n",
    "    sims[user2] = sim\n",
    "    #print()\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores for Nieble\n",
      "{'Nieble': 1.0, 'DQ': 0.2853838065194532, 'Odyssey': 0.09849624060150376, 'Awaken': 0.10057769219374907, 'BB': 0.10232886379675371}\n",
      "\n",
      "Similarity scores for DQ\n",
      "{'Nieble': 0.2853838065194532, 'DQ': 1.0, 'Odyssey': 0.08160466312360706, 'Awaken': 0.08170813718897109, 'BB': 0.08693571542510767}\n",
      "\n",
      "Similarity scores for Odyssey\n",
      "{'Nieble': 0.09849624060150376, 'DQ': 0.08160466312360706, 'Odyssey': 1.0, 'Awaken': 0.2907429345066847, 'BB': 0.3098315066252249}\n",
      "\n",
      "Similarity scores for Awaken\n",
      "{'Nieble': 0.10057769219374907, 'DQ': 0.08170813718897109, 'Odyssey': 0.2907429345066847, 'Awaken': 1.0, 'BB': 0.3135196252624778}\n",
      "\n",
      "Similarity scores for BB\n",
      "{'Nieble': 0.10232886379675371, 'DQ': 0.08693571542510767, 'Odyssey': 0.3098315066252249, 'Awaken': 0.3135196252624778, 'BB': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sims = {}\n",
    "docs =[\"Nieble\",\"DQ\",\"Odyssey\",\"Awaken\",\"BB\"]\n",
    "def simScores(docName): #based off of nb3sol\n",
    "    for user in df_charMatrix.columns:\n",
    "        col1 = docName\n",
    "        col2 = user\n",
    "        numer = np.sum((df_charMatrix[col1]==1) & (df_charMatrix[col2]==1))\n",
    "        denom = np.sum((df_charMatrix[col1]==1) | (df_charMatrix[col2]==1))\n",
    "        sim = numer/denom\n",
    "        sims[user] = sim\n",
    "    print(sims)\n",
    "for i in range(len(docs)):\n",
    "    print(\"Similarity scores for \"+ docs[i])\n",
    "    simScores(docs[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Implement minhashing with 1000 hash functions on the 4 documents, checking your results against those in part b).\n",
    "\n",
    "- You may choose your own value of $p$ as the modulus of the hash functions.  You are encouraged to use the example code from the minhashing in class notebook to start you out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash1(nhash, dfC): #-------------------------------------------funcction taken nb03 (changed pHash value)\n",
    "    '''\n",
    "    Takes a number of hash functions to use (nhash) and characteristic matrix (dfC)\n",
    "    '''\n",
    "    # use the \"universal hash\":  (a*x+b) mod p, where a, b are random ints and p > N (= 10 here) is prime\n",
    "    np.random.seed(4022)\n",
    "    Ahash = np.random.choice(range(0,10000), size=nhash)\n",
    "    Bhash = np.random.choice(range(0,10000), size=nhash)\n",
    "    Phash =10069  # 1439\n",
    "\n",
    "    # STEP 2:  initialize signature matrix to all infinities\n",
    "\n",
    "    # initialize the signature matrix\n",
    "    Msig = np.full([nhash, len(dfC.columns)], fill_value=np.inf)\n",
    "\n",
    "    # fill in the signature matrix:\n",
    "\n",
    "    # For each row of the characteristic matrix... \n",
    "    hash_vals = [0]*nhash # initialize\n",
    "    for r in range(len(dfC)):\n",
    "        # STEP 3:  Compute hash values (~permuted row numbers) for that row under each hash function\n",
    "        for h in range(nhash):\n",
    "            hash_vals[h] = (Ahash[h]*r + Bhash[h])%Phash\n",
    "        # STEP 4:  For each column, if there is a 0, do nothing...\n",
    "        for c in range(len(dfC.columns)):\n",
    "            # ... but if there is a 1, replace signature matrix element in that column for each hash fcn \n",
    "            # with the minimum of the hash value in this row, and the current signature matrix element\n",
    "            if dfC.iloc[r,c]==1:\n",
    "                for h in range(nhash):\n",
    "                    if hash_vals[h] < Msig[h,c]:\n",
    "                        Msig[h,c] = hash_vals[h]\n",
    "    return Msig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nHash =1000\n",
    "Msig = minhash1(nHash, df_charMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(Msig) #puts in df\n",
    "#df1.head(10005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.319, 0.138, 0.121, 0.118]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sum(Msig[:,0]==Msig[:,k])/nHash for k in range(len(df1.columns))] #single case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are similarities with minHashing\n",
      "[1.0, 0.319, 0.138, 0.121, 0.118]\n",
      "[0.319, 1.0, 0.104, 0.106, 0.103]\n",
      "[0.138, 0.104, 1.0, 0.323, 0.331]\n",
      "[0.121, 0.106, 0.323, 1.0, 0.345]\n",
      "[0.118, 0.103, 0.331, 0.345, 1.0]\n",
      "Compared to the exact similarities...\n",
      "______________________________________________\n",
      "Similarity scores for Nieble\n",
      "{'Nieble': 1.0, 'DQ': 0.2853838065194532, 'Odyssey': 0.09849624060150376, 'Awaken': 0.10057769219374907, 'BB': 0.10232886379675371}\n",
      "\n",
      "Similarity scores for DQ\n",
      "{'Nieble': 0.2853838065194532, 'DQ': 1.0, 'Odyssey': 0.08160466312360706, 'Awaken': 0.08170813718897109, 'BB': 0.08693571542510767}\n",
      "\n",
      "Similarity scores for Odyssey\n",
      "{'Nieble': 0.09849624060150376, 'DQ': 0.08160466312360706, 'Odyssey': 1.0, 'Awaken': 0.2907429345066847, 'BB': 0.3098315066252249}\n",
      "\n",
      "Similarity scores for Awaken\n",
      "{'Nieble': 0.10057769219374907, 'DQ': 0.08170813718897109, 'Odyssey': 0.2907429345066847, 'Awaken': 1.0, 'BB': 0.3135196252624778}\n",
      "\n",
      "Similarity scores for BB\n",
      "{'Nieble': 0.10232886379675371, 'DQ': 0.08693571542510767, 'Odyssey': 0.3098315066252249, 'Awaken': 0.3135196252624778, 'BB': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nhash=1000\n",
    "\n",
    "def helper(index,Msig,nhash):\n",
    "    sims2 =[sum(Msig[:,index]==Msig[:,k])/nHash for k in range(len(df1.columns))]\n",
    "    #for i in range(5):\n",
    "        #sims2 ={}\n",
    "        #print(sum(Msig[:,index]==Msig[:,i]))\n",
    "    #    sim = sum(Msig[:,index]==Msig[:,i])/nhash\n",
    "    #    sims2.append(sim)\n",
    "    print(sims2)\n",
    "print(\"These are similarities with minHashing\")\n",
    "for i in range(5):\n",
    "    helper(i,Msig,nHash)\n",
    "\n",
    "print(\"Compared to the exact similarities...\")\n",
    "print(\"______________________________________________\")\n",
    "for i in range(len(docs)):\n",
    "    print(\"Similarity scores for \"+ docs[i])\n",
    "    simScores(docs[i])\n",
    "    print(\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to note that if we increase the nHashes we would get a more precise similarity, however I think above is a good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### d) Discussion:\n",
    "\n",
    "Can we detect expected differences here?  Are the two Spanish docuemnts most similar to each other?  Are the two documents by the same author, with the same theme, the most similar?  What kind of alternatives might have captured the structures between these texts?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes we can detect differences here, we can see either through minhashing or direct jaccard similarity that the document Nieble is around 30% similar to the DQ doc. So to answer the question, yes the two spanish documents are most similar to each other. The awakening document and BB document share the same author and also have similarity of around 30% to each other. It seems the general theme of individual growth in the Awakening and Beyond the Bayou have a similar structures to that as spirtual growth in The Odyssey. The documents Odyseey and Awakening share similarity of around 33%, Beyond the Bayou and the Odyseey share around a 34% similarity. Of course there could be more alternatives that affect the structure between the texts such as when was the text written and maybe even the context of how much of the document we are comparing. An instance of this would be if we compare one document's introduction verse another documents body paragraph, even if those are same length it could still have effect on similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
